{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ6SNYq_tVVC"
   },
   "source": [
    "# Classify text with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCjmX4zTCkRK"
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w_XlxN1IsRJ"
   },
   "source": [
    "Using AdamW optimizer from [tensorflow/models](https://github.com/tensorflow/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b-P1ZOA0FkVJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple/\n",
      "Collecting torch\n",
      "  Using cached torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.11.0\n",
      "Looking in indexes: https://pypi.python.org/simple/\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.4.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: tokenizers, regex, numpy, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.0 huggingface-hub-0.6.0 numpy-1.22.3 regex-2022.4.24 tokenizers-0.12.1 transformers-4.19.2\n",
      "Looking in indexes: https://pypi.python.org/simple/\n",
      "Collecting torchtext\n",
      "  Using cached torchtext-0.12.0-cp38-cp38-manylinux1_x86_64.whl (10.4 MB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchtext) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from torchtext) (4.62.3)\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.8/site-packages (from torchtext) (1.11.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchtext) (1.22.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.11.0->torchtext) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext) (2.0.12)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.12.0\n"
     ]
    }
   ],
   "source": [
    "# First install all necessary packages\n",
    "\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install -U torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple/\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.4.2 pytz-2022.1\n",
      "Looking in indexes: https://pypi.python.org/simple/\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (1.22.3)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-9.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.33.3 kiwisolver-1.4.2 matplotlib-3.5.2 pillow-9.1.1\n",
      "Looking in indexes: https://pypi.python.org/simple/\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.22.3)\n",
      "Collecting scipy>=1.3.2\n",
      "  Using cached scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
      "Collecting joblib>=1.0.0\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.1.0 scipy-1.8.1 sklearn-0.0 threadpoolctl-3.1.0\n",
      "Looking in indexes: https://pypi.python.org/simple/\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement official (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for official\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_XgTpm9ZxoN9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from utils import load_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# from official.nlp import optimization  # to create AdamW optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/jovyan/workbench-shared-folder/workbench-shared-folder/canary-project/Paula_internship/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnvd4mrtPHHV"
   },
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pOdqCMoQDRJL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.83 Mb (83.6% reduction)\n",
      "Mem. usage decreased to  2.42 Mb (83.6% reduction)\n",
      "Mem. usage decreased to  0.81 Mb (83.6% reduction)\n",
      "Set A with suffix '_kw' was loaded successfully.\n",
      "Mem. usage decreased to  5.18 Mb (83.6% reduction)\n",
      "Mem. usage decreased to  2.59 Mb (83.6% reduction)\n",
      "Mem. usage decreased to  0.86 Mb (83.6% reduction)\n",
      "Set B with suffix '_kw' was loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workbench-shared-folder/canary-project/Paula_internship/utils.py:46: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  train = pd.read_csv(f\"{path}set_{version}_train{suffix}.csv\", engine='python', error_bad_lines=False)\n",
      "Skipping line 157634: unexpected end of data\n",
      "/home/jovyan/workbench-shared-folder/canary-project/Paula_internship/utils.py:47: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  test = pd.read_csv(f\"{path}set_{version}_test{suffix}.csv\", engine='python', error_bad_lines=False)\n",
      "/home/jovyan/workbench-shared-folder/canary-project/Paula_internship/utils.py:48: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  val = pd.read_csv(f\"{path}set_{version}_val{suffix}.csv\", engine='python', error_bad_lines=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 10.82 Mb (83.6% reduction)\n",
      "Mem. usage decreased to  5.42 Mb (83.6% reduction)\n",
      "Mem. usage decreased to  1.81 Mb (83.6% reduction)\n",
      "Set EX with suffix '_kw' was loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# DATA_PATH = \"../data/\"\n",
    "DATA_PATH = \"/home/jovyan/workbench-shared-folder/canary-project/Paula_internship/data/\"\n",
    "\n",
    "# Load data from Set A, B and EX\n",
    "train_A, test_A, val_A = load_data(DATA_PATH, version=\"A\", suffix=\"_kw\", reduce_memory=True)\n",
    "train_B, test_B, val_B = load_data(DATA_PATH, version=\"B\", suffix=\"_kw\", reduce_memory=True)\n",
    "train_EX, test_EX, val_EX = load_data(DATA_PATH, version=\"EX\", suffix=\"_kw\", reduce_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We concatenate the 3 different sets (A, B, EX):\n",
    "\n",
    "train = pd.concat([train_A, train_B, train_EX])\n",
    "test = pd.concat([test_A, test_B, test_EX])\n",
    "val = pd.concat([val_A, val_B, val_EX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Classifier using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    '''\n",
    "    Dataset class to map indices/keys of data samples. Implemented __getitem__() and __len__() protocols.\n",
    "    Using the tokenizer, the inputs are mapped to BERT ids/mask.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_len, truncate):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.truncation = truncate\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        inputs = self.tokenizer.__call__(text,\n",
    "                                        None,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=self.max_len,\n",
    "                                        padding=\"max_length\",\n",
    "                                        truncation=self.truncation,\n",
    "                                        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "#             \"length\": \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# load\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "input_text = \"Here is some text to encode\"\n",
    "# tokenizer-> token_id\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "# input_ids: [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]\n",
    "input_ids = torch.tensor([input_ids])\n",
    "\n",
    "with torch.no_grad():\n",
    "    gg = model(input_ids)\n",
    "    last_hidden_states = model(input_ids)[0] \n",
    "# The last_hidden_states are a tensor of shape (batch_size, sequence_length, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example, the text “Here is some text to encode”:\n",
    "- The text gets tokenized into 9 tokens (the input_ids) - actually 7 but 2 special tokens are added, namely [CLS] at the start and [SEP] at the end. So the sequence length is 9.\n",
    "- The batch size is 1, as we only forward a single sentence through the model.\n",
    "- And the hidden_size of a BERT-base-sized model is 768.\n",
    "\n",
    "**Hence, the last hidden states have shape (1, 9, 768).**\n",
    "\n",
    "\n",
    "We can then get the last hidden state vector of each token, e.g. if you want to get it for the first token, you would have to type last_hidden_states[:,0,:]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0549])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg[0][:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer.__call__(input_text,None,add_special_tokens=True, max_length=200, padding=\"max_length\", truncation=True)\n",
    "idsss = tok['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    '''\n",
    "    The actual NN used for classification\n",
    "    '''\n",
    "    def __init__(self, n_train_steps, n_classes, do_prob, bert_model):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(do_prob)\n",
    "        self.out = nn.Linear(768, n_classes)\n",
    "        self.n_train_steps = n_train_steps\n",
    "        self.step_scheduler_after = \"batch\"\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        output_1 = self.bert(ids, attention_mask=mask)[\"pooler_output\"]\n",
    "        output_2 = self.dropout(output_1)\n",
    "        output = self.out(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, n_train_steps, n_classes, do_prob, bert_model, dimension=128):\n",
    "        super(LSTM_Classifier, self).__init__()\n",
    "#         self.embedding = nn.Embedding(len(text_field.vocab), 200)\n",
    "        self.bert = bert_model\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=200,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=do_prob)\n",
    "\n",
    "        self.fc = nn.Linear(2*dimension, n_classes)\n",
    "        \n",
    "    def forward(self, ids, mask, text_len=200):\n",
    "\n",
    "        text_emb = self.bert(ids, attention_mask=mask)[\"pooler_output\"]\n",
    "        \n",
    "        # IS IT NECESSARY TO PACK ??\n",
    "#         packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_input = text_emb\n",
    "        output, _ = self.lstm(packed_input)\n",
    "#         output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output[:, 0, self.dimension:] # Take only CLS token\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1) # Concatenates the given sequence of seq tensors in dim 1.\n",
    "        text_fea = self.drop(out_reduced) # Dropout\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, n_classes)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "\n",
    "        return text_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define which BERT we are using:\n",
    "\n",
    "# # SPECTER: Document-level Representation Learning using Citation-informed Transformers\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"allenai/specter\", do_lower_case=True)\n",
    "# bert_model = transformers.AutoModel.from_pretrained(\"allenai/specter\")\n",
    "\n",
    "# # SCIBERT: BERT model trained on scientific text.\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=True)\n",
    "# bert_model = transformers.AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "\n",
    "# # SQUEEZE BERT: \n",
    "# tokenizer = transformers.SqueezeBertTokenizer.from_pretrained(\"squeezebert/squeezebert-uncased\", do_lower_case=True)\n",
    "# bert_model = transformers.SqueezeBertModel.from_pretrained(\"squeezebert/squeezebert-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# FREEZE BERT PARAMS OR FINE-TUNE?\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=True)\n",
    "bert_model = transformers.AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels= train.iloc[:, 3:].shape[1]\n",
    "\n",
    "def build_dataset(tokenizer_max_len, truncate):\n",
    "    '''\n",
    "    Tokenize and map the training and validation sets\n",
    "    '''\n",
    "    train_dataset = Dataset(train.input.tolist(), train.iloc[:, 3:].values.tolist(), tokenizer, tokenizer_max_len, truncate)\n",
    "    valid_dataset = Dataset(val.input.tolist(), val.iloc[:, 3:].values.tolist(), tokenizer, tokenizer_max_len, truncate)\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def build_dataloader(train_dataset, valid_dataset, batch_size):\n",
    "    '''\n",
    "    Create the torch dataloaders\n",
    "    '''\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    return train_data_loader, valid_data_loader\n",
    "\n",
    "def build_bucketiterator(train_dataset, valid_dataset, batch_size):\n",
    "    '''\n",
    "    Create the torch BucketIterators\n",
    "    '''\n",
    "    train_data_loader = BucketIterator(train_dataset, batch_size=batch_size, sort_key=lambda x: len(x.ids),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, sort_key=lambda x: len(x.ids),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "\n",
    "    return train_data_loader, valid_data_loader\n",
    "\n",
    "def ret_model(n_train_steps, do_prob):\n",
    "    '''\n",
    "    Retrieve the model\n",
    "    '''\n",
    "    model = LSTM_Classifier(n_train_steps, n_labels, do_prob, bert_model=bert_model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, valid_dataset = build_dataset(config['tokenizer_max_len'], config['truncate'])\n",
    "# train_data_loader, valid_data_loader = build_dataloader(train_dataset, valid_dataset, config['batch_size'])\n",
    "# train_bucket, valid_bucket = build_bucketiterator(train_dataset, valid_dataset, config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_optimizer(model):\n",
    "    '''\n",
    "    Taken from Abhishek Thakur's Tez library example: \n",
    "    https://github.com/abhishekkrthakur/tez/blob/main/examples/text_classification/binary.py\n",
    "    '''\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "#     opt = AdamW(optimizer_parameters, lr=config['learning_rate'])\n",
    "    opt = torch.optim.AdamW(optimizer_parameters, lr=config['learning_rate'])\n",
    "    return opt\n",
    "\n",
    "def ret_scheduler(optimizer, num_train_steps):\n",
    "    sch = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    return sch\n",
    "\n",
    "def loss_function(outputs, labels, loss='BCE'):\n",
    "    if labels is None:\n",
    "        return None\n",
    "    if loss == 'BCE':\n",
    "        # BinaryCross Entropy loss\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        return loss_fn(outputs, labels.float())\n",
    "    elif loss == 'SigF1':\n",
    "        S=-1\n",
    "        E=0\n",
    "        y_hat = torch.sigmoid(outputs)\n",
    "        y = labels\n",
    "        # Sigmoid hyperparams:\n",
    "        b = torch.tensor(S)\n",
    "        c = torch.tensor(E)\n",
    "\n",
    "        # Calculate the sigmoid\n",
    "        sig = 1 / (1 + torch.exp(b * (y_hat + c)))\n",
    "        tp = torch.sum(sig * y, dim=0)\n",
    "        fp = torch.sum(sig * (1 - y), dim=0)\n",
    "        fn = torch.sum((1 - sig) * y, dim=0)\n",
    "\n",
    "        sigmoid_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "        cost = 1 - sigmoid_f1\n",
    "        macroCost = torch.mean(cost)\n",
    "\n",
    "        return macroCost\n",
    "    else:\n",
    "        # BinaryCross Entropy loss\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "        return loss_fn(outputs, labels.float())\n",
    "\n",
    "def log_metrics(preds, labels):\n",
    "    preds = torch.stack(preds)\n",
    "    preds = preds.cpu().detach().numpy()\n",
    "    labels = torch.stack(labels)\n",
    "    labels = labels.cpu().detach().numpy()\n",
    "    \n",
    "    '''\n",
    "    auc_micro_list = []\n",
    "    for i in range(n_labels):\n",
    "      current_pred = preds.T[i]\n",
    "      current_label = labels.T[i]\n",
    "      fpr_micro, tpr_micro, _ = metrics.roc_curve(current_label.T, current_pred.T)\n",
    "      auc_micro = metrics.auc(fpr_micro, tpr_micro)\n",
    "      auc_micro_list.append(auc_micro)\n",
    "    \n",
    "    return {\"auc\": np.array(auc_micro).mean()}\n",
    "    '''\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "    fpr_micro, tpr_micro, _ = metrics.roc_curve(labels.ravel(), preds.ravel())\n",
    "    \n",
    "    auc_micro = metrics.auc(fpr_micro, tpr_micro)\n",
    "    return {\"auc_micro\": auc_micro}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_fn(data_loader, model, loss_fn, optimizer, device, scheduler):\n",
    "    '''\n",
    "        Modified from Abhishek Thakur's BERT example: \n",
    "        https://github.com/abhishekkrthakur/bert-sentiment/blob/master/src/engine.py\n",
    "    '''\n",
    "\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        ids = d[\"ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets = d[\"labels\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids=ids, mask=mask)\n",
    "\n",
    "        loss = loss_function(outputs, targets, loss_fn)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return train_loss\n",
    "    \n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    '''\n",
    "        Modified from Abhishek Thakur's BERT example: \n",
    "        https://github.com/abhishekkrthakur/bert-sentiment/blob/master/src/engine.py\n",
    "    '''\n",
    "    eval_loss = 0.0\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            ids = d[\"ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets = d[\"labels\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(ids=ids, mask=mask)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            eval_loss += loss.item()\n",
    "            fin_targets.extend(targets)\n",
    "            fin_outputs.extend(torch.sigmoid(outputs))\n",
    "    return eval_loss, fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(config):\n",
    "\n",
    "    train_dataset, valid_dataset = build_dataset(config['tokenizer_max_len'], config['truncate'])\n",
    "    train_data_loader, valid_data_loader = build_dataloader(train_dataset, valid_dataset, config['batch_size'])\n",
    "    print(\"Length of Train Dataloader: \", len(train_data_loader))\n",
    "    print(\"Length of Valid Dataloader: \", len(valid_data_loader))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    n_train_steps = int(len(train_dataset) / config['batch_size'] * 10)\n",
    "\n",
    "    model = ret_model(n_train_steps, config['dropout'])\n",
    "    optimizer = ret_optimizer(model)\n",
    "    scheduler = ret_scheduler(optimizer, n_train_steps)\n",
    "    model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "    n_epochs = config['epochs']\n",
    "    loss_fn = config['loss']\n",
    "\n",
    "    best_val_loss = 100\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        print('Train EPOCH: ', epoch+1)\n",
    "        train_loss = train_fn(train_data_loader, model, loss_fn, optimizer, device, scheduler)\n",
    "        eval_loss, preds, labels = eval_fn(valid_data_loader, model, device)\n",
    "        \n",
    "        metrics_eval = log_metrics(preds, labels)\n",
    "        try:\n",
    "            auc_score  = metrics_eval[\"auc_micro\"]\n",
    "#             print(\"AUC score: \", auc_score)\n",
    "        except:\n",
    "            pass\n",
    "        avg_train_loss, avg_val_loss = train_loss / len(train_data_loader), eval_loss / len(valid_data_loader)\n",
    "\n",
    "        print(\"Average Train loss: \", avg_train_loss)\n",
    "        print(\"Average Valid loss: \", avg_val_loss)\n",
    "        torch.save(model.state_dict(), \"./models/model_current_LSTM.pt\")  \n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"./models/model_best_LSTM.pt\")  \n",
    "            print(\"Model saved as current val_loss is: \", best_val_loss)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some configuration parameters (to be fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 10,\n",
    "    'dropout': 0.3,\n",
    "    'tokenizer_max_len': 200,\n",
    "    'truncate': True,\n",
    "    'loss': 'BCE',\n",
    "#     'loss': 'SigF1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Dataloader:  4743\n",
      "Length of Valid Dataloader:  792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5162b09c8ffe4fb8b4a057a2ba09010e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EPOCH:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536525e0dce34b3590d64ae4c20cdb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model:\n",
    "\n",
    "trainer(config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions for the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    '''\n",
    "    Load a saved model\n",
    "    '''\n",
    "    train_dataset, valid_dataset = build_dataset(config['tokenizer_max_len'], config['truncate'])\n",
    "    train_data_loader, valid_data_loader = build_dataloader(train_dataset, valid_dataset, config['batch_size'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    n_train_steps = int(len(train_dataset) / config['batch_size'] * 10)\n",
    "\n",
    "    model = ret_model(n_train_steps, config['dropout'])\n",
    "    optimizer = ret_optimizer(model)\n",
    "    scheduler = ret_scheduler(optimizer, n_train_steps)\n",
    "    model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    Models_PATH = \"/home/jovyan/workbench-shared-folder/canary-project/Paula_internship/models/\"\n",
    "    model.load_state_dict(torch.load(Models_PATH + model_name, map_location=device))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_batches(test, model):\n",
    "    '''\n",
    "    Predict outputs for inference phase\n",
    "    '''\n",
    "    test_targets = []\n",
    "    test_outputs = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    test_dataset = Dataset(test.input.tolist(), test.iloc[:, 3:].values.tolist(), tokenizer, config['tokenizer_max_len'], config['truncate'])\n",
    "    data_loader = DataLoader(test_dataset, batch_size=1024, shuffle=True, num_workers=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            ids = d[\"ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            labels = d[\"labels\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(ids=ids, mask=mask)\n",
    "            test_targets.extend(labels.cpu().numpy())\n",
    "            test_outputs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "\n",
    "\n",
    "    return test_outputs, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates():\n",
    "    '''\n",
    "    Remove duplicates of train/val datasets present in the test set\n",
    "    '''\n",
    "    # Get the training duplicates:\n",
    "    duplicates_train = set(test.pui) & set(train.pui) \n",
    "    test_clean = test[~test['pui'].isin(duplicates_train)]\n",
    "    \n",
    "    # Get the validation duplicates:\n",
    "    duplicates_val = set(test.pui) & set(val.pui) \n",
    "    test_clean = test_clean[~test_clean['pui'].isin(duplicates_val)]\n",
    "    \n",
    "    assert test_clean.shape[0] == test.shape[0] - len(duplicates_train) - len(duplicates_val)\n",
    "    \n",
    "    return test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(preds, labels):\n",
    "    '''\n",
    "    Create some metrics: precison, recall, F1...\n",
    "    '''\n",
    "    # Convert the lists to dataframes\n",
    "    lab_df = pd.DataFrame(labels)\n",
    "    pred_df = pd.DataFrame(preds).round(0).astype(int)\n",
    "    \n",
    "    # Calculate tp/fp/fn/tn per class:\n",
    "    tp = (pred_df + lab_df).eq(2).sum()\n",
    "    fp = (pred_df - lab_df).eq(1).sum()\n",
    "    fn = (pred_df - lab_df).eq(-1).sum()\n",
    "    tn = (pred_df + lab_df).eq(0).sum()\n",
    "    \n",
    "    # Calculate precision and recall:\n",
    "    prec = [tp[i] / (tp[i] + fp[i]) * 100.0 if tp[i] + fp[i] != 0 else 0.0 for i in range(len(tp))]\n",
    "    rec = [tp[i] / (tp[i] + fn[i]) * 100.0 if tp[i] + fn[i] != 0 else 0.0 for i in range(len(tp))]\n",
    "    \n",
    "    # Calculate F1 score:\n",
    "    f1_score = [2 * prec[i] * rec[i] / (prec[i] + rec[i]) if tp[i] > 0 else 0.0 for i in range(len(tp))]\n",
    "    \n",
    "    # Weighted F1 score:\n",
    "    weight = lab_df.sum() / sum(lab_df.sum())\n",
    "    f1_wght = [weight[i] * 2 * prec[i] * rec[i] / (prec[i] + rec[i]) if tp[i] > 0 else 0.0 for i in range(len(tp))]\n",
    "    \n",
    "    # Macro average:\n",
    "    prec_avg = sum(prec) / len(prec)\n",
    "    rec_avg = sum(rec) / len(rec)\n",
    "    f1_avg = sum(f1_score) / len(f1_score)\n",
    "    f1wgt_avg = sum(f1_wght) / len(f1_wght)\n",
    "    \n",
    "    return {\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1 score': f1_score,\n",
    "        'Weighted F1 score': f1_wght,\n",
    "        'weights': weight,\n",
    "        'Average precision': prec_avg.round(2),\n",
    "        'Average recall': rec_avg.round(2),\n",
    "        'Average F1 score': f1_avg.round(2),\n",
    "        'Average weighted F1 score': f1wgt_avg.round(2),\n",
    "    }\n",
    "\n",
    "# all_metrics = get_metrics(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict outputs:\n",
    "test_clean = remove_duplicates()\n",
    "model = load_model('model_best_freezeF1.pt')\n",
    "preds, labels = inference_batches(test_clean, model)\n",
    "all_metrics = get_metrics(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = get_metrics(preds, labels)\n",
    "\n",
    "for metr, val in all_metrics.items():\n",
    "    if 'Average' in metr:\n",
    "        print(metr, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame([all_metrics['Precision'], all_metrics['Recall'], all_metrics['F1 score'], all_metrics['weights']*100]).rename(index={0: \"Precision\", 1: \"Recall\", 2: \"F1\", 3: \"Weights\"}).T\n",
    "# metrics_df = metrics_df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n",
    "metrics_df.to_csv('outputs/metrics_FrozenSci_F1.csv', sep=';')\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
